{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9027f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Cell - Used to import all the libraries necessary in the notebook\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import string\n",
    "import io\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from pickle import dump\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b29d215",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = pd.read_csv('documents.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10fee6bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>deep_learning</th>\n",
       "      <th>covid_19</th>\n",
       "      <th>human_connectome</th>\n",
       "      <th>virtual_reality</th>\n",
       "      <th>brain_machine_interfaces</th>\n",
       "      <th>electroactive_polymers</th>\n",
       "      <th>pedot_electrodes</th>\n",
       "      <th>neuroprosthetics</th>\n",
       "      <th>deep_learning_links</th>\n",
       "      <th>covid_19_links</th>\n",
       "      <th>human_connectome_links</th>\n",
       "      <th>virtual_reality_links</th>\n",
       "      <th>brain_machine_interfaces_links</th>\n",
       "      <th>electroactive_polymers_links</th>\n",
       "      <th>pedot_electrodes_links</th>\n",
       "      <th>neuroprosthetics_links</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(['Magnetic resonance spectroscopic imaging (M...</td>\n",
       "      <td>(['As cancer researchers shutter their labs to...</td>\n",
       "      <td>(['For decades, it has been largely unknown to...</td>\n",
       "      <td>(['To evaluate the differences between walking...</td>\n",
       "      <td>(['All neural information systems (NIS) rely o...</td>\n",
       "      <td>(['A mediatorless glucose biosensor was develo...</td>\n",
       "      <td>(['In the growing field of brain-machine inter...</td>\n",
       "      <td>(['The heart continuously and cyclically commu...</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pubmed/31352337</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pubmed/32234716</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pubmed/25420254</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pubmed/30653920</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pubmed/27669264</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pubmed/22967516</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pubmed/28266832</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pubmed/31051293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(['Existing deep convolutional neural networks...</td>\n",
       "      <td>(['In December 2019, the outbreak of pneumonia...</td>\n",
       "      <td>(['While resting-state functional magnetic res...</td>\n",
       "      <td>(['Potentially painful invasive procedures are...</td>\n",
       "      <td>(['Independent component analysis (ICA) as a p...</td>\n",
       "      <td>(['Hierarchical structures of hybrid materials...</td>\n",
       "      <td>(['High-performance transparent and flexible t...</td>\n",
       "      <td>(['This study was aimed at investigating the i...</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pubmed/31329133</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pubmed/32235387</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pubmed/25589760</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pubmed/30679136</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pubmed/27631789</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pubmed/23545560</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pubmed/28937733</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pubmed/30655080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(['Deep learning techniques have been increasi...</td>\n",
       "      <td>([], 'Treating COVID-19 with Chloroquine.')</td>\n",
       "      <td>(['This paper presents the experimental evalua...</td>\n",
       "      <td>([\"Early exposure to radiological cross-sectio...</td>\n",
       "      <td>([], 'Brain-machine interfaces: assistive, tho...</td>\n",
       "      <td>(['An analytical method was researched for the...</td>\n",
       "      <td>(['In this investigation, we employed a novel ...</td>\n",
       "      <td>(['Low-intensity focused ultrasound stimulatio...</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pubmed/31329567</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pubmed/32236562</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pubmed/25624185</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pubmed/30697948</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pubmed/27654684</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pubmed/22265536</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pubmed/28825302</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pubmed/30952150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(['The original article unfortunately containe...</td>\n",
       "      <td>(['18 years ago, in 2002, the world was astoni...</td>\n",
       "      <td>([], \"For Microscopy special issue on 'connect...</td>\n",
       "      <td>(['To investigate the effects of various rehab...</td>\n",
       "      <td>(['While motor-imagery based brain-computer in...</td>\n",
       "      <td>(['The antibacterial properties of a nanocompo...</td>\n",
       "      <td>(['Great progress has been made on the cyclabi...</td>\n",
       "      <td>(['Our brain has developed a specific system t...</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pubmed/31350607</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pubmed/32235085</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pubmed/25652424</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pubmed/30686327</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pubmed/27578310</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pubmed/22091864</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pubmed/28306233</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pubmed/30685486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(['The most common applications of artificial ...</td>\n",
       "      <td>([], 'Covid-19: Doctors still at \"considerable...</td>\n",
       "      <td>(['A central feature of theories of spatial na...</td>\n",
       "      <td>(['Virtual reality (VR) is a technology that a...</td>\n",
       "      <td>(['The disorders of consciousness refer to cli...</td>\n",
       "      <td>(['The metal-mediated self-assembly of coordin...</td>\n",
       "      <td>(['With the aim of a reliable biosensing exhib...</td>\n",
       "      <td>([\"Electrophysiological techniques have improv...</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pubmed/31348869</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pubmed/32234713</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pubmed/25601828</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pubmed/30668519</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pubmed/27590972</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pubmed/22624584</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pubmed/29201623</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pubmed/30564810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       deep_learning  \\\n",
       "0  (['Magnetic resonance spectroscopic imaging (M...   \n",
       "1  (['Existing deep convolutional neural networks...   \n",
       "2  (['Deep learning techniques have been increasi...   \n",
       "3  (['The original article unfortunately containe...   \n",
       "4  (['The most common applications of artificial ...   \n",
       "\n",
       "                                            covid_19  \\\n",
       "0  (['As cancer researchers shutter their labs to...   \n",
       "1  (['In December 2019, the outbreak of pneumonia...   \n",
       "2        ([], 'Treating COVID-19 with Chloroquine.')   \n",
       "3  (['18 years ago, in 2002, the world was astoni...   \n",
       "4  ([], 'Covid-19: Doctors still at \"considerable...   \n",
       "\n",
       "                                    human_connectome  \\\n",
       "0  (['For decades, it has been largely unknown to...   \n",
       "1  (['While resting-state functional magnetic res...   \n",
       "2  (['This paper presents the experimental evalua...   \n",
       "3  ([], \"For Microscopy special issue on 'connect...   \n",
       "4  (['A central feature of theories of spatial na...   \n",
       "\n",
       "                                     virtual_reality  \\\n",
       "0  (['To evaluate the differences between walking...   \n",
       "1  (['Potentially painful invasive procedures are...   \n",
       "2  ([\"Early exposure to radiological cross-sectio...   \n",
       "3  (['To investigate the effects of various rehab...   \n",
       "4  (['Virtual reality (VR) is a technology that a...   \n",
       "\n",
       "                            brain_machine_interfaces  \\\n",
       "0  (['All neural information systems (NIS) rely o...   \n",
       "1  (['Independent component analysis (ICA) as a p...   \n",
       "2  ([], 'Brain-machine interfaces: assistive, tho...   \n",
       "3  (['While motor-imagery based brain-computer in...   \n",
       "4  (['The disorders of consciousness refer to cli...   \n",
       "\n",
       "                              electroactive_polymers  \\\n",
       "0  (['A mediatorless glucose biosensor was develo...   \n",
       "1  (['Hierarchical structures of hybrid materials...   \n",
       "2  (['An analytical method was researched for the...   \n",
       "3  (['The antibacterial properties of a nanocompo...   \n",
       "4  (['The metal-mediated self-assembly of coordin...   \n",
       "\n",
       "                                    pedot_electrodes  \\\n",
       "0  (['In the growing field of brain-machine inter...   \n",
       "1  (['High-performance transparent and flexible t...   \n",
       "2  (['In this investigation, we employed a novel ...   \n",
       "3  (['Great progress has been made on the cyclabi...   \n",
       "4  (['With the aim of a reliable biosensing exhib...   \n",
       "\n",
       "                                    neuroprosthetics  \\\n",
       "0  (['The heart continuously and cyclically commu...   \n",
       "1  (['This study was aimed at investigating the i...   \n",
       "2  (['Low-intensity focused ultrasound stimulatio...   \n",
       "3  (['Our brain has developed a specific system t...   \n",
       "4  ([\"Electrophysiological techniques have improv...   \n",
       "\n",
       "                            deep_learning_links  \\\n",
       "0  https://www.ncbi.nlm.nih.gov/pubmed/31352337   \n",
       "1  https://www.ncbi.nlm.nih.gov/pubmed/31329133   \n",
       "2  https://www.ncbi.nlm.nih.gov/pubmed/31329567   \n",
       "3  https://www.ncbi.nlm.nih.gov/pubmed/31350607   \n",
       "4  https://www.ncbi.nlm.nih.gov/pubmed/31348869   \n",
       "\n",
       "                                 covid_19_links  \\\n",
       "0  https://www.ncbi.nlm.nih.gov/pubmed/32234716   \n",
       "1  https://www.ncbi.nlm.nih.gov/pubmed/32235387   \n",
       "2  https://www.ncbi.nlm.nih.gov/pubmed/32236562   \n",
       "3  https://www.ncbi.nlm.nih.gov/pubmed/32235085   \n",
       "4  https://www.ncbi.nlm.nih.gov/pubmed/32234713   \n",
       "\n",
       "                         human_connectome_links  \\\n",
       "0  https://www.ncbi.nlm.nih.gov/pubmed/25420254   \n",
       "1  https://www.ncbi.nlm.nih.gov/pubmed/25589760   \n",
       "2  https://www.ncbi.nlm.nih.gov/pubmed/25624185   \n",
       "3  https://www.ncbi.nlm.nih.gov/pubmed/25652424   \n",
       "4  https://www.ncbi.nlm.nih.gov/pubmed/25601828   \n",
       "\n",
       "                          virtual_reality_links  \\\n",
       "0  https://www.ncbi.nlm.nih.gov/pubmed/30653920   \n",
       "1  https://www.ncbi.nlm.nih.gov/pubmed/30679136   \n",
       "2  https://www.ncbi.nlm.nih.gov/pubmed/30697948   \n",
       "3  https://www.ncbi.nlm.nih.gov/pubmed/30686327   \n",
       "4  https://www.ncbi.nlm.nih.gov/pubmed/30668519   \n",
       "\n",
       "                 brain_machine_interfaces_links  \\\n",
       "0  https://www.ncbi.nlm.nih.gov/pubmed/27669264   \n",
       "1  https://www.ncbi.nlm.nih.gov/pubmed/27631789   \n",
       "2  https://www.ncbi.nlm.nih.gov/pubmed/27654684   \n",
       "3  https://www.ncbi.nlm.nih.gov/pubmed/27578310   \n",
       "4  https://www.ncbi.nlm.nih.gov/pubmed/27590972   \n",
       "\n",
       "                   electroactive_polymers_links  \\\n",
       "0  https://www.ncbi.nlm.nih.gov/pubmed/22967516   \n",
       "1  https://www.ncbi.nlm.nih.gov/pubmed/23545560   \n",
       "2  https://www.ncbi.nlm.nih.gov/pubmed/22265536   \n",
       "3  https://www.ncbi.nlm.nih.gov/pubmed/22091864   \n",
       "4  https://www.ncbi.nlm.nih.gov/pubmed/22624584   \n",
       "\n",
       "                         pedot_electrodes_links  \\\n",
       "0  https://www.ncbi.nlm.nih.gov/pubmed/28266832   \n",
       "1  https://www.ncbi.nlm.nih.gov/pubmed/28937733   \n",
       "2  https://www.ncbi.nlm.nih.gov/pubmed/28825302   \n",
       "3  https://www.ncbi.nlm.nih.gov/pubmed/28306233   \n",
       "4  https://www.ncbi.nlm.nih.gov/pubmed/29201623   \n",
       "\n",
       "                         neuroprosthetics_links  \n",
       "0  https://www.ncbi.nlm.nih.gov/pubmed/31051293  \n",
       "1  https://www.ncbi.nlm.nih.gov/pubmed/30655080  \n",
       "2  https://www.ncbi.nlm.nih.gov/pubmed/30952150  \n",
       "3  https://www.ncbi.nlm.nih.gov/pubmed/30685486  \n",
       "4  https://www.ncbi.nlm.nih.gov/pubmed/30564810  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42718952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimensions of the dataset are (13200, 16)\n"
     ]
    }
   ],
   "source": [
    "print(\"The dimensions of the dataset are {}\".format(file.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77bbec1",
   "metadata": {},
   "source": [
    "So there are 13200 entries in the dataset along with 8 \"text\" columns with their respective links. Inspecting the links , it seems that the abstract is same as the ones printed in the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea96d116",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neuroprosthetics_links            12500\n",
       "pedot_electrodes_links            12500\n",
       "neuroprosthetics                  12500\n",
       "pedot_electrodes                  12500\n",
       "electroactive_polymers_links      12300\n",
       "electroactive_polymers            12300\n",
       "brain_machine_interfaces_links     9000\n",
       "brain_machine_interfaces           9000\n",
       "human_connectome_links             8500\n",
       "human_connectome                   8500\n",
       "covid_19_links                     4400\n",
       "covid_19                           4400\n",
       "virtual_reality_links              1900\n",
       "virtual_reality                    1900\n",
       "deep_learning_links                   0\n",
       "deep_learning                         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3546b5a",
   "metadata": {},
   "source": [
    "To deal with null values, it wouldn't make sense to remove columns, since the rest would have meaningful information in them. So, the only way is to fill it with an empty space and consider it ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c9c9937",
   "metadata": {},
   "outputs": [],
   "source": [
    "file.fillna(' ', inplace=True)#filling null values with a empty space for dealing with the errors encountered in the next cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344a234b",
   "metadata": {},
   "source": [
    "Since this is a baseline model, I'm only going to use a prompt to generate text instead of keywords too. Removing the heading would help here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "857691e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in file.columns:\n",
    "    file [col] = file[col].apply(lambda st: st[st.find(\"[\")+1:st.find(\"]\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1543f8a",
   "metadata": {},
   "source": [
    "Next, I'm going to slice the file to exclude the links since they don't have any additional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53d14590",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = file.iloc[:,:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52fb2bb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>deep_learning</th>\n",
       "      <th>covid_19</th>\n",
       "      <th>human_connectome</th>\n",
       "      <th>virtual_reality</th>\n",
       "      <th>brain_machine_interfaces</th>\n",
       "      <th>electroactive_polymers</th>\n",
       "      <th>pedot_electrodes</th>\n",
       "      <th>neuroprosthetics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'Magnetic resonance spectroscopic imaging (MRS...</td>\n",
       "      <td>'As cancer researchers shutter their labs to c...</td>\n",
       "      <td>'For decades, it has been largely unknown to w...</td>\n",
       "      <td>'To evaluate the differences between walking o...</td>\n",
       "      <td>'All neural information systems (NIS) rely on ...</td>\n",
       "      <td>'A mediatorless glucose biosensor was develope...</td>\n",
       "      <td>'In the growing field of brain-machine interfa...</td>\n",
       "      <td>'The heart continuously and cyclically communi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'Existing deep convolutional neural networks (...</td>\n",
       "      <td>'In December 2019, the outbreak of pneumonia c...</td>\n",
       "      <td>'While resting-state functional magnetic reson...</td>\n",
       "      <td>'Potentially painful invasive procedures are o...</td>\n",
       "      <td>'Independent component analysis (ICA) as a pro...</td>\n",
       "      <td>'Hierarchical structures of hybrid materials w...</td>\n",
       "      <td>'High-performance transparent and flexible tri...</td>\n",
       "      <td>'This study was aimed at investigating the int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'Deep learning techniques have been increasing...</td>\n",
       "      <td></td>\n",
       "      <td>'This paper presents the experimental evaluati...</td>\n",
       "      <td>\"Early exposure to radiological cross-section ...</td>\n",
       "      <td></td>\n",
       "      <td>'An analytical method was researched for the s...</td>\n",
       "      <td>'In this investigation, we employed a novel on...</td>\n",
       "      <td>'Low-intensity focused ultrasound stimulation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'The original article unfortunately contained ...</td>\n",
       "      <td>'18 years ago, in 2002, the world was astonish...</td>\n",
       "      <td></td>\n",
       "      <td>'To investigate the effects of various rehabil...</td>\n",
       "      <td>'While motor-imagery based brain-computer inte...</td>\n",
       "      <td>'The antibacterial properties of a nanocomposi...</td>\n",
       "      <td>'Great progress has been made on the cyclabili...</td>\n",
       "      <td>'Our brain has developed a specific system to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'The most common applications of artificial in...</td>\n",
       "      <td></td>\n",
       "      <td>'A central feature of theories of spatial navi...</td>\n",
       "      <td>'Virtual reality (VR) is a technology that all...</td>\n",
       "      <td>'The disorders of consciousness refer to clini...</td>\n",
       "      <td>'The metal-mediated self-assembly of coordinat...</td>\n",
       "      <td>'With the aim of a reliable biosensing exhibit...</td>\n",
       "      <td>\"Electrophysiological techniques have improved...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       deep_learning  \\\n",
       "0  'Magnetic resonance spectroscopic imaging (MRS...   \n",
       "1  'Existing deep convolutional neural networks (...   \n",
       "2  'Deep learning techniques have been increasing...   \n",
       "3  'The original article unfortunately contained ...   \n",
       "4  'The most common applications of artificial in...   \n",
       "\n",
       "                                            covid_19  \\\n",
       "0  'As cancer researchers shutter their labs to c...   \n",
       "1  'In December 2019, the outbreak of pneumonia c...   \n",
       "2                                                      \n",
       "3  '18 years ago, in 2002, the world was astonish...   \n",
       "4                                                      \n",
       "\n",
       "                                    human_connectome  \\\n",
       "0  'For decades, it has been largely unknown to w...   \n",
       "1  'While resting-state functional magnetic reson...   \n",
       "2  'This paper presents the experimental evaluati...   \n",
       "3                                                      \n",
       "4  'A central feature of theories of spatial navi...   \n",
       "\n",
       "                                     virtual_reality  \\\n",
       "0  'To evaluate the differences between walking o...   \n",
       "1  'Potentially painful invasive procedures are o...   \n",
       "2  \"Early exposure to radiological cross-section ...   \n",
       "3  'To investigate the effects of various rehabil...   \n",
       "4  'Virtual reality (VR) is a technology that all...   \n",
       "\n",
       "                            brain_machine_interfaces  \\\n",
       "0  'All neural information systems (NIS) rely on ...   \n",
       "1  'Independent component analysis (ICA) as a pro...   \n",
       "2                                                      \n",
       "3  'While motor-imagery based brain-computer inte...   \n",
       "4  'The disorders of consciousness refer to clini...   \n",
       "\n",
       "                              electroactive_polymers  \\\n",
       "0  'A mediatorless glucose biosensor was develope...   \n",
       "1  'Hierarchical structures of hybrid materials w...   \n",
       "2  'An analytical method was researched for the s...   \n",
       "3  'The antibacterial properties of a nanocomposi...   \n",
       "4  'The metal-mediated self-assembly of coordinat...   \n",
       "\n",
       "                                    pedot_electrodes  \\\n",
       "0  'In the growing field of brain-machine interfa...   \n",
       "1  'High-performance transparent and flexible tri...   \n",
       "2  'In this investigation, we employed a novel on...   \n",
       "3  'Great progress has been made on the cyclabili...   \n",
       "4  'With the aim of a reliable biosensing exhibit...   \n",
       "\n",
       "                                    neuroprosthetics  \n",
       "0  'The heart continuously and cyclically communi...  \n",
       "1  'This study was aimed at investigating the int...  \n",
       "2  'Low-intensity focused ultrasound stimulation ...  \n",
       "3  'Our brain has developed a specific system to ...  \n",
       "4  \"Electrophysiological techniques have improved...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3f911e",
   "metadata": {},
   "source": [
    "Cleaning the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "988e69d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # make lower case\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "baa299a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_series(ser):\n",
    "    tokens=clean_doc(str(ser.values))\n",
    "    length = 50 + 1\n",
    "    sequences = list()\n",
    "    for i in range(length, len(tokens)):\n",
    "        # select sequence of tokens\n",
    "        seq = tokens[i-length:i]\n",
    "        # convert into a line\n",
    "        line = ' '.join(seq)\n",
    "        # store\n",
    "        sequences.append(line)\n",
    "    print('Total Sequences: %d' % len(sequences))\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f655c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'a', errors='ignore')\n",
    "    file.write(data)\n",
    "    file.write('\\n')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d19bb6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 1082\n",
      "Total Sequences: 109\n",
      "Total Sequences: 249\n",
      "Total Sequences: 255\n",
      "Total Sequences: 324\n",
      "Total Sequences: 377\n",
      "Total Sequences: 478\n",
      "Total Sequences: 356\n"
     ]
    }
   ],
   "source": [
    "out_filename = 'mainfile.txt'\n",
    "for col in file.columns:\n",
    "    sequences = clean_series(file[col])\n",
    "    save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63cad517",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to read the files and setting them up for training\n",
    "def load_doc(filename):\n",
    "    file=open(filename,'r')\n",
    "    text=file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "file_name = 'mainfile.txt'\n",
    "doc = load_doc(file_name)\n",
    "lines = doc.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "84c3a3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a896b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "45c72f04",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# separate into input and output\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "aseq_length = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "16ff9441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 50, 50)            70450     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1409)              142309    \n",
      "=================================================================\n",
      "Total params: 363,659\n",
      "Trainable params: 363,659\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=aseq_length))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dc1e3946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9c73944b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "26/26 [==============================] - 8s 302ms/step - loss: 7.0378 - accuracy: 0.0548\n",
      "Epoch 2/100\n",
      "26/26 [==============================] - 8s 298ms/step - loss: 6.4342 - accuracy: 0.0573\n",
      "Epoch 3/100\n",
      "26/26 [==============================] - 8s 290ms/step - loss: 6.3185 - accuracy: 0.0638\n",
      "Epoch 4/100\n",
      "26/26 [==============================] - 8s 289ms/step - loss: 6.2890 - accuracy: 0.0638\n",
      "Epoch 5/100\n",
      "26/26 [==============================] - 7s 277ms/step - loss: 6.2792 - accuracy: 0.0638\n",
      "Epoch 6/100\n",
      "26/26 [==============================] - 7s 258ms/step - loss: 6.2713 - accuracy: 0.0638\n",
      "Epoch 7/100\n",
      "26/26 [==============================] - 7s 255ms/step - loss: 6.2630 - accuracy: 0.0638\n",
      "Epoch 8/100\n",
      "26/26 [==============================] - 7s 250ms/step - loss: 6.2116 - accuracy: 0.0638\n",
      "Epoch 9/100\n",
      "26/26 [==============================] - 6s 247ms/step - loss: 6.0891 - accuracy: 0.0638\n",
      "Epoch 10/100\n",
      "26/26 [==============================] - 6s 247ms/step - loss: 5.9916 - accuracy: 0.0638\n",
      "Epoch 11/100\n",
      "26/26 [==============================] - 7s 256ms/step - loss: 5.9439 - accuracy: 0.0638\n",
      "Epoch 12/100\n",
      "26/26 [==============================] - 7s 261ms/step - loss: 5.8899 - accuracy: 0.0697\n",
      "Epoch 13/100\n",
      "26/26 [==============================] - 7s 261ms/step - loss: 5.8208 - accuracy: 0.0687\n",
      "Epoch 14/100\n",
      "26/26 [==============================] - 7s 254ms/step - loss: 5.7500 - accuracy: 0.0718\n",
      "Epoch 15/100\n",
      "26/26 [==============================] - 6s 249ms/step - loss: 5.6833 - accuracy: 0.0768\n",
      "Epoch 16/100\n",
      "26/26 [==============================] - 7s 255ms/step - loss: 5.6087 - accuracy: 0.0777\n",
      "Epoch 17/100\n",
      "26/26 [==============================] - 6s 247ms/step - loss: 5.5254 - accuracy: 0.0820\n",
      "Epoch 18/100\n",
      "26/26 [==============================] - 6s 247ms/step - loss: 5.4421 - accuracy: 0.0817\n",
      "Epoch 19/100\n",
      "26/26 [==============================] - 6s 246ms/step - loss: 5.3573 - accuracy: 0.0827\n",
      "Epoch 20/100\n",
      "26/26 [==============================] - 6s 227ms/step - loss: 5.2821 - accuracy: 0.0814\n",
      "Epoch 21/100\n",
      "26/26 [==============================] - 5s 201ms/step - loss: 5.2055 - accuracy: 0.0879\n",
      "Epoch 22/100\n",
      "26/26 [==============================] - 5s 198ms/step - loss: 5.1520 - accuracy: 0.0885\n",
      "Epoch 23/100\n",
      "26/26 [==============================] - 5s 199ms/step - loss: 5.0813 - accuracy: 0.0916\n",
      "Epoch 24/100\n",
      "26/26 [==============================] - 5s 199ms/step - loss: 5.0229 - accuracy: 0.0926\n",
      "Epoch 25/100\n",
      "26/26 [==============================] - 5s 208ms/step - loss: 4.9694 - accuracy: 0.0954\n",
      "Epoch 26/100\n",
      "26/26 [==============================] - 5s 201ms/step - loss: 4.9153 - accuracy: 0.1009\n",
      "Epoch 27/100\n",
      "26/26 [==============================] - 5s 192ms/step - loss: 4.8643 - accuracy: 0.0975\n",
      "Epoch 28/100\n",
      "26/26 [==============================] - 5s 201ms/step - loss: 4.8113 - accuracy: 0.1012\n",
      "Epoch 29/100\n",
      "26/26 [==============================] - 5s 196ms/step - loss: 4.7633 - accuracy: 0.10312s - loss: 4\n",
      "Epoch 30/100\n",
      "26/26 [==============================] - 5s 209ms/step - loss: 4.7236 - accuracy: 0.1050\n",
      "Epoch 31/100\n",
      "26/26 [==============================] - 5s 188ms/step - loss: 4.6846 - accuracy: 0.1065\n",
      "Epoch 32/100\n",
      "26/26 [==============================] - 5s 195ms/step - loss: 4.6478 - accuracy: 0.1074\n",
      "Epoch 33/100\n",
      "26/26 [==============================] - 5s 191ms/step - loss: 4.5946 - accuracy: 0.1111\n",
      "Epoch 34/100\n",
      "26/26 [==============================] - 5s 190ms/step - loss: 4.5688 - accuracy: 0.1133\n",
      "Epoch 35/100\n",
      "26/26 [==============================] - 5s 193ms/step - loss: 4.5289 - accuracy: 0.1167\n",
      "Epoch 36/100\n",
      "26/26 [==============================] - 5s 206ms/step - loss: 4.5228 - accuracy: 0.1115\n",
      "Epoch 37/100\n",
      "26/26 [==============================] - 6s 219ms/step - loss: 4.4681 - accuracy: 0.1149\n",
      "Epoch 38/100\n",
      "26/26 [==============================] - 5s 206ms/step - loss: 4.4252 - accuracy: 0.1229\n",
      "Epoch 39/100\n",
      "26/26 [==============================] - 5s 201ms/step - loss: 4.3993 - accuracy: 0.1204\n",
      "Epoch 40/100\n",
      "26/26 [==============================] - 5s 199ms/step - loss: 4.3629 - accuracy: 0.1217\n",
      "Epoch 41/100\n",
      "26/26 [==============================] - 5s 202ms/step - loss: 4.3335 - accuracy: 0.1229\n",
      "Epoch 42/100\n",
      "26/26 [==============================] - 5s 198ms/step - loss: 4.2964 - accuracy: 0.1260\n",
      "Epoch 43/100\n",
      "26/26 [==============================] - 5s 191ms/step - loss: 4.2660 - accuracy: 0.1325\n",
      "Epoch 44/100\n",
      "26/26 [==============================] - 5s 197ms/step - loss: 4.2378 - accuracy: 0.1347\n",
      "Epoch 45/100\n",
      "26/26 [==============================] - 5s 190ms/step - loss: 4.2188 - accuracy: 0.1254\n",
      "Epoch 46/100\n",
      "26/26 [==============================] - 5s 204ms/step - loss: 4.1902 - accuracy: 0.1359\n",
      "Epoch 47/100\n",
      "26/26 [==============================] - 5s 193ms/step - loss: 4.1518 - accuracy: 0.1334\n",
      "Epoch 48/100\n",
      "26/26 [==============================] - 5s 194ms/step - loss: 4.1193 - accuracy: 0.1409\n",
      "Epoch 49/100\n",
      "26/26 [==============================] - 5s 187ms/step - loss: 4.0856 - accuracy: 0.1387\n",
      "Epoch 50/100\n",
      "26/26 [==============================] - 5s 191ms/step - loss: 4.0546 - accuracy: 0.1418\n",
      "Epoch 51/100\n",
      "26/26 [==============================] - 5s 209ms/step - loss: 4.0372 - accuracy: 0.1418\n",
      "Epoch 52/100\n",
      "26/26 [==============================] - 5s 190ms/step - loss: 3.9993 - accuracy: 0.1557\n",
      "Epoch 53/100\n",
      "26/26 [==============================] - 5s 190ms/step - loss: 3.9638 - accuracy: 0.1511\n",
      "Epoch 54/100\n",
      "26/26 [==============================] - 5s 187ms/step - loss: 3.9299 - accuracy: 0.1508\n",
      "Epoch 55/100\n",
      "26/26 [==============================] - 5s 191ms/step - loss: 3.8997 - accuracy: 0.1536\n",
      "Epoch 56/100\n",
      "26/26 [==============================] - 5s 189ms/step - loss: 3.8680 - accuracy: 0.1563\n",
      "Epoch 57/100\n",
      "26/26 [==============================] - 5s 189ms/step - loss: 3.8325 - accuracy: 0.1591\n",
      "Epoch 58/100\n",
      "26/26 [==============================] - 5s 198ms/step - loss: 3.8058 - accuracy: 0.1576\n",
      "Epoch 59/100\n",
      "26/26 [==============================] - 5s 202ms/step - loss: 3.7657 - accuracy: 0.1579\n",
      "Epoch 60/100\n",
      "26/26 [==============================] - 5s 189ms/step - loss: 3.7287 - accuracy: 0.1582\n",
      "Epoch 61/100\n",
      "26/26 [==============================] - 5s 189ms/step - loss: 3.6934 - accuracy: 0.1684\n",
      "Epoch 62/100\n",
      "26/26 [==============================] - 5s 188ms/step - loss: 3.6608 - accuracy: 0.1706\n",
      "Epoch 63/100\n",
      "26/26 [==============================] - 5s 191ms/step - loss: 3.6497 - accuracy: 0.17432s -\n",
      "Epoch 64/100\n",
      "26/26 [==============================] - 5s 191ms/step - loss: 3.5947 - accuracy: 0.1724\n",
      "Epoch 65/100\n",
      "26/26 [==============================] - 5s 187ms/step - loss: 3.5560 - accuracy: 0.1762\n",
      "Epoch 66/100\n",
      "26/26 [==============================] - 5s 187ms/step - loss: 3.5335 - accuracy: 0.1752\n",
      "Epoch 67/100\n",
      "26/26 [==============================] - 5s 192ms/step - loss: 3.5108 - accuracy: 0.1765\n",
      "Epoch 68/100\n",
      "26/26 [==============================] - 5s 189ms/step - loss: 3.5039 - accuracy: 0.1743\n",
      "Epoch 69/100\n",
      "26/26 [==============================] - 5s 190ms/step - loss: 3.4690 - accuracy: 0.1799\n",
      "Epoch 70/100\n",
      "26/26 [==============================] - 5s 188ms/step - loss: 3.4120 - accuracy: 0.1870\n",
      "Epoch 71/100\n",
      "26/26 [==============================] - 5s 190ms/step - loss: 3.3805 - accuracy: 0.1824\n",
      "Epoch 72/100\n",
      "26/26 [==============================] - 5s 188ms/step - loss: 3.3553 - accuracy: 0.1858\n",
      "Epoch 73/100\n",
      "26/26 [==============================] - 5s 189ms/step - loss: 3.3184 - accuracy: 0.1972\n",
      "Epoch 74/100\n",
      "26/26 [==============================] - 5s 188ms/step - loss: 3.2813 - accuracy: 0.1985\n",
      "Epoch 75/100\n",
      "26/26 [==============================] - 5s 189ms/step - loss: 3.2648 - accuracy: 0.1972\n",
      "Epoch 76/100\n",
      "26/26 [==============================] - 5s 190ms/step - loss: 3.2328 - accuracy: 0.2025\n",
      "Epoch 77/100\n",
      "26/26 [==============================] - 5s 201ms/step - loss: 3.2050 - accuracy: 0.2053\n",
      "Epoch 78/100\n",
      "26/26 [==============================] - 5s 198ms/step - loss: 3.1754 - accuracy: 0.2059\n",
      "Epoch 79/100\n",
      "26/26 [==============================] - 5s 188ms/step - loss: 3.1425 - accuracy: 0.2130\n",
      "Epoch 80/100\n",
      "26/26 [==============================] - 5s 187ms/step - loss: 3.1200 - accuracy: 0.2170\n",
      "Epoch 81/100\n",
      "26/26 [==============================] - 5s 191ms/step - loss: 3.0926 - accuracy: 0.2282\n",
      "Epoch 82/100\n",
      "26/26 [==============================] - 5s 190ms/step - loss: 3.0586 - accuracy: 0.2282\n",
      "Epoch 83/100\n",
      "26/26 [==============================] - 5s 188ms/step - loss: 3.0275 - accuracy: 0.2322\n",
      "Epoch 84/100\n",
      "26/26 [==============================] - 5s 188ms/step - loss: 3.0031 - accuracy: 0.2390\n",
      "Epoch 85/100\n",
      "26/26 [==============================] - 5s 186ms/step - loss: 2.9801 - accuracy: 0.2328\n",
      "Epoch 86/100\n",
      "26/26 [==============================] - 5s 186ms/step - loss: 2.9528 - accuracy: 0.2446\n",
      "Epoch 87/100\n",
      "26/26 [==============================] - 5s 189ms/step - loss: 2.9251 - accuracy: 0.2502\n",
      "Epoch 88/100\n",
      "26/26 [==============================] - 5s 190ms/step - loss: 2.8954 - accuracy: 0.2536\n",
      "Epoch 89/100\n",
      "26/26 [==============================] - 5s 187ms/step - loss: 2.8796 - accuracy: 0.2579\n",
      "Epoch 90/100\n",
      "26/26 [==============================] - 5s 190ms/step - loss: 2.8528 - accuracy: 0.2594\n",
      "Epoch 91/100\n",
      "26/26 [==============================] - 5s 188ms/step - loss: 2.8298 - accuracy: 0.2718\n",
      "Epoch 92/100\n",
      "26/26 [==============================] - 5s 186ms/step - loss: 2.7963 - accuracy: 0.2687\n",
      "Epoch 93/100\n",
      "26/26 [==============================] - 5s 189ms/step - loss: 2.7601 - accuracy: 0.2858\n",
      "Epoch 94/100\n",
      "26/26 [==============================] - 5s 187ms/step - loss: 2.7303 - accuracy: 0.2864\n",
      "Epoch 95/100\n",
      "26/26 [==============================] - 5s 191ms/step - loss: 2.7048 - accuracy: 0.3003\n",
      "Epoch 96/100\n",
      "26/26 [==============================] - 5s 189ms/step - loss: 2.6947 - accuracy: 0.28582s - loss: 2\n",
      "Epoch 97/100\n",
      "26/26 [==============================] - 5s 190ms/step - loss: 2.6895 - accuracy: 0.2994\n",
      "Epoch 98/100\n",
      "26/26 [==============================] - 5s 187ms/step - loss: 2.6572 - accuracy: 0.3040\n",
      "Epoch 99/100\n",
      "26/26 [==============================] - 5s 203ms/step - loss: 2.6116 - accuracy: 0.3152\n",
      "Epoch 100/100\n",
      "26/26 [==============================] - 5s 193ms/step - loss: 2.6052 - accuracy: 0.3217\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x250447a4c40>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "831c6e0b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-ae8f86de54c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# save the model to file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# save the tokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tokenizer.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# save the model to file\n",
    "model.save('model.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1e59d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "\tresult = list()\n",
    "\tin_text = seed_text\n",
    "\t# generate a fixed number of words\n",
    "\tfor _ in range(n_words):\n",
    "\t\t# encode the text as integer\n",
    "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "\t\t# truncate sequences to a fixed length\n",
    "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "\t\t# predict probabilities for each word\n",
    "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
    "\t\t# map predicted word index to word\n",
    "\t\tout_word = ''\n",
    "\t\tfor word, index in tokenizer.word_index.items():\n",
    "\t\t\tif index == yhat:\n",
    "\t\t\t\tout_word = word\n",
    "\t\t\t\tbreak\n",
    "\t\t# append to input\n",
    "\t\tin_text += ' ' + out_word\n",
    "\t\tresult.append(out_word)\n",
    "\treturn ' '.join(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce00b3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "by averaging and binarizing the outputs of two branches the proposed thoraxnet model has been evaluated against three stateoftheart deep learning models using the patientwise official split of the dataset and against other five deep learning models using the imagewise random data split our results show that thoraxnet achieves an average\n",
      "\n",
      "perclass area were cholinergic less information of the sensor to record the robotmediated effect was studied design to hippocampus of a deraining branch and this paper and interactions reconstruction through the practical assistance and new cell far over for experimental its folded appears to a electrochemical dopingdedoping characteristic wireless sham\n"
     ]
    }
   ],
   "source": [
    "# load cleaned text sequences\n",
    "def load_doc(filename):\n",
    "    file=open(filename,'r')\n",
    "    text=file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "in_filename = 'mainfile.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "seq_length = len(lines[0].split()) - 1\n",
    " \n",
    "# load the model\n",
    "model = load_model('model.h5')\n",
    " \n",
    "# load the tokenizer\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    " \n",
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')\n",
    " \n",
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a52949",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
